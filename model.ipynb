{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dotenv\n",
    "from transformers import LongformerTokenizer, LongformerModel\n",
    "import torch\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain import LLMChain\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "\n",
    "# Load environment variables\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Longformer model and tokenizer\n",
    "tokenizer = LongformerTokenizer.from_pretrained('allenai/longformer-large-4096')\n",
    "model = LongformerModel.from_pretrained('allenai/longformer-large-4096')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom embedding class with embed_query method\n",
    "class LongformerEmbedding:\n",
    "    def __init__(self, tokenizer, model):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model = model\n",
    "\n",
    "    def embed_query(self, text):\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\", max_length=4096, truncation=True)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "            embedding = outputs.last_hidden_state.mean(dim=1).squeeze().tolist()  # Mean pooling\n",
    "        return embedding\n",
    "\n",
    "# Instantiate the custom embedding class\n",
    "longformer_embedder = LongformerEmbedding(tokenizer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to Pinecone and initialize with the existing index\n",
    "index_name = \"hubspot-crm-txts\"\n",
    "\n",
    "# Initialize PineconeVectorStore with the custom embedding class\n",
    "vector_store = PineconeVectorStore.from_existing_index(\n",
    "    index_name=index_name,\n",
    "    embedding=longformer_embedder  # Pass the embedding class here\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the Retrieval Chain with LangChain\n",
    "llm = ChatAnthropic(model='claude-3-opus-20240229', temperature=0.1)\n",
    "\n",
    "# Define your custom prompt with context and prefix\n",
    "template = \"\"\"You are a helpful assistant that generates HubSpot API code based on the provided context.\n",
    "Always prioritize information from the context when available.\n",
    "\n",
    "Context: {text}\n",
    "\n",
    "Generate the Python code using the HubSpot Client Library with no comments to answer the following question.\n",
    "Use your general knowledge as a helpful assistant if no specific context is provided.\n",
    "Only return code, no additional text. Use the HubSpot Python library where possible.\n",
    "You will be provided with the access_token so be sure to use it. \n",
    "Note that the returned response from your code should be a json object, do not parse it. \n",
    "Your final line should be: print(response), where response is the json object returned from your API call.\n",
    "Do not add a 'limit' parameter within the response unless explicitly asked.\n",
    "If you are asked to filter the data by a specific property, you can create a Filter from the Hubspot Python library. \n",
    "Don't forget to import the proper library as well based on the HubSpot object in question.\n",
    "Be sure to wrap the code in a try catch block and print the error if any.\n",
    "\n",
    "Question: {question}\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"text\", \"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Retrieval-Augmented Generation (RAG) chain\n",
    "rag_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=vector_store.as_retriever(search_kwargs={\"k\": 3}),\n",
    "    chain_type=\"stuff\",\n",
    "    chain_type_kwargs = {\n",
    "        \"prompt\": prompt,\n",
    "        \"document_variable_name\": \"text\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "try:\n",
      "    from hubspot import HubSpot\n",
      "    from hubspot.crm.companies import ApiException, Filter, FilterGroup, PublicObjectSearchRequest\n",
      "    \n",
      "    hubspot = HubSpot(access_token='your_access_token')\n",
      "    \n",
      "    filter = Filter(property_name=\"domain\", operator=\"EQ\", value=\"hubspot.com\")\n",
      "    filter_group = FilterGroup(filters=[filter])\n",
      "    public_object_search_request = PublicObjectSearchRequest(filter_groups=[filter_group])\n",
      "    \n",
      "    response = hubspot.crm.companies.search_api.do_search(public_object_search_request=public_object_search_request)\n",
      "    \n",
      "    print(response)\n",
      "except ApiException as e:\n",
      "    print(\"Error: {0}\".format(e))\n"
     ]
    }
   ],
   "source": [
    "# Query the RAG Chain\n",
    "query = \"what company has the website hubspot.com\"\n",
    "response = rag_chain.invoke(query)\n",
    "print(response['result'])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
