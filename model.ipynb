{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dotenv\n",
    "from transformers import LongformerTokenizer, LongformerModel\n",
    "import torch\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain import LLMChain\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "\n",
    "# Load environment variables\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Longformer model and tokenizer\n",
    "tokenizer = LongformerTokenizer.from_pretrained('allenai/longformer-large-4096')\n",
    "model = LongformerModel.from_pretrained('allenai/longformer-large-4096')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom embedding class with embed_query method\n",
    "class LongformerEmbedding:\n",
    "    def __init__(self, tokenizer, model):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model = model\n",
    "\n",
    "    def embed_query(self, text):\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\", max_length=4096, truncation=True)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "            embedding = outputs.last_hidden_state.mean(dim=1).squeeze().tolist()  # Mean pooling\n",
    "        return embedding\n",
    "\n",
    "# Instantiate the custom embedding class\n",
    "longformer_embedder = LongformerEmbedding(tokenizer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to Pinecone and initialize with the existing index\n",
    "index_name = \"hubspot-crm-txts\"\n",
    "\n",
    "# Initialize PineconeVectorStore with the custom embedding class\n",
    "vector_store = PineconeVectorStore.from_existing_index(\n",
    "    index_name=index_name,\n",
    "    embedding=longformer_embedder  # Pass the embedding class here\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the Retrieval Chain with LangChain\n",
    "llm = ChatAnthropic(model='claude-3-opus-20240229', temperature=0.1)\n",
    "\n",
    "# Define your custom prompt with context and prefix\n",
    "template = \"\"\"You are a helpful assistant that generates HubSpot API code based on the provided context.\n",
    "Always prioritize information from the context when available.\n",
    "\n",
    "Context: {text}\n",
    "\n",
    "Generate the Python code using the HubSpot Client Library with no comments to answer the following question.\n",
    "Use your general knowledge as a helpful assistant if no specific context is provided.\n",
    "Only return code, no additional text. Use the HubSpot Python library where possible.\n",
    "You will be provided with the access_token so be sure to use it. \n",
    "Note that the returned response from your code should be a json object, do not parse it. Your final line should be: print(response) where response is the json object returned from your API call.\n",
    "Do not add a 'limit' parameter within the response unless explicitly asked.\n",
    "\n",
    "Question: {question}\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"text\", \"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Retrieval-Augmented Generation (RAG) chain\n",
    "rag_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=vector_store.as_retriever(search_kwargs={\"k\": 3}),\n",
    "    chain_type=\"stuff\",\n",
    "    chain_type_kwargs = {\n",
    "        \"prompt\": prompt,\n",
    "        \"document_variable_name\": \"text\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from hubspot import HubSpot\n",
      "\n",
      "access_token = 'YOUR_ACCESS_TOKEN'\n",
      "\n",
      "api_client = HubSpot(access_token=access_token)\n",
      "\n",
      "response = api_client.crm.contacts.basic_api.get_page(properties=[\"firstname\", \"lastname\"])\n",
      "\n",
      "print(response)\n"
     ]
    }
   ],
   "source": [
    "# Query the RAG Chain\n",
    "query = \"what are the names of all the contacts?\"\n",
    "response = rag_chain.invoke(query)\n",
    "print(response['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
